---
title: "Bayesian Sample Size Estimation for Multilevel Trials"
author: "Ulrich LÃ¶sener"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
---

# Simulation study for the project: Bayesian Sample Size Determination for Multilevel Trials


```{r Packages, message=FALSE}
library(tidyr)       # pipes
library(dplyr)       # pipes
library(ggplot2)     # plots
library(lme4)        # fit multilevel model
library(mgcv)        # extracting vcov matrices
library(bain)        # Bayesian estimation
library(MASS)        # multinorm - already included in lme4?
library(latex2exp)   # latex notation in plots
```


```{r Function SSD}

SSD <- function(m=100, t.points=c(1,2,3,4,5), var.u0=.0333, var.u1=.1, var.e=.0262, eff.size=.8, BFthres=3){
  source("fct_data_generation.R")
  
  start <- Sys.time()
  eta <- .8          # desired power
  N <- 30            # initial N
  condition <- FALSE # condition fulfillment indicator

  while (condition==F) {
    
    N <- N+2
    results <- dat.gen(m=m, N=N, t.points=t.points, var.u0=var.u0, var.u1=var.u1, var.e=var.e, eff.size=eff.size, BFthres=BFthres)
    # condition met?
    ifelse(results$Prop_BF0>eta & results$Prop_BF1>eta, condition <- TRUE, condition <- FALSE)
    print(N)
    
  }
  print(Sys.time() - start) 

}

```


```{r Function SSD Binary}

SSD.binary <- function(m=100, t.points=c(1,2,3,4,5), var.u0=.0333, var.u1=.1, var.e=.0262, eff.size=.8, BFthres=3){
  source("fct_data_generation.R")
  
  start <- Sys.time()
  eta <- .8          # desired power
  N.current <- 30    # initial N
  N.min <- 20
  N.max <- 1000
  condition <- FALSE # condition fulfillment indicator

  while (N.current-N.min>1) {
    
    N.current <- round((N.min + N.max)/2, digits = 0)

    results <- dat.gen(m=m, N=N.current, t.points=t.points, var.u0=var.u0, var.u1=var.u1, var.e=var.e, eff.size=eff.size, BFthres=BFthres)
    # condition met?
    ifelse(results$Prop_BF0>eta & results$Prop_BF1>eta, condition <- TRUE, condition <- FALSE)
    ifelse(condition==FALSE, N.min <- N.current, N.max <- N.current)
    print(N.current)
    
  }
  print(Sys.time() - start) 

}

```
  
  
  
```{r Data Generation}

#bayesian.SSD <- function(m = 1000, eff.size = .8, BF.thresh = 3, t.points = c(0,1,2,3,4), eta=.8) {
  
  start <- Sys.time()

#for(u in 3:10){
  n.steps <- 200  # number of different sample sizes to evaluate
  m <- 100      # number of datasets
  eff.size <- .8 # effect size (Cohen's d): .2 (small), .5 (medium), .8 (large)
  BF.thresh <- 3  # desired Bayes Factor
  eta <- .8      # probability with which a BF of at least BF.tresh is to be achieved, equivalent to power
  
  N <- 30
  
  t.points <- c(1:5) # time of measurements
  n <- length(t.points)    # number of measurements per subject
  
  # Make N as N per group
  # alternative BF is just inverse , delete PMPs 
  
  # prop.bf10.H0 <- numeric(n.steps) # storage for proportions of BFs exceeding a certain threshold
  prop.bf01.H0 <- numeric(n.steps) # storage for proportions of BFs exceeding a certain threshold
  # medbf10.H0 <- numeric(n.steps) # storage for median BFs
  medbf01.H0 <- numeric(n.steps) # storage for median BFs
  medbf.1u.H0 <- numeric(n.steps) # storage for median BFs

  
  prop.bf10.H1 <- numeric(n.steps) # storage for proportions of BFs exceeding a certain threshold
  # prop.bf01.H1 <- numeric(n.steps) # storage for proportions of BFs exceeding a certain threshold
  medbf10.H1 <- numeric(n.steps) # storage for median BFs
  # medbf01.H1 <- numeric(n.steps) # storage for median BFs
  medbf.1u.H1 <- numeric(n.steps) # storage for median BFs

  ss.seq <- rep(NA, n.steps)
  
  set.seed(1234)      # for reproducibility
  #seeds <- vector("list", n.steps) 
  
  sigmasq.u0 <- .0333   # variance of individual deviation from treatment intercept 
  sigmasq.u1 <- 1   # variance of individual deviation from treatment slope
  sigma.u0.u1 <- 0  # covariance between sigmasq.u0 and sigmasq.u1. If positive, then individuals with higher(lower) initial values tend to have a higher (lower) rate of change over time.
  sigmasq.e <- .0262    # error variance
  
  # bf10.H0 <- rep(NA,m) # storage for BFs
  bf01.H0 <- rep(NA,m) # storage for BFs
  bf.1u.H0 <- rep(NA,m) # storage for BFs

  
  bf10.H1 <- rep(NA,m) # storage for BFs
  # bf01.H1 <- rep(NA,m) # storage for BFs
  bf.1u.H1 <- rep(NA,m) # storage for BFs

  
 for(j in 1:n.steps){

  
    N <- j+29     # number of subjects
    ss.seq[j] <- N

    
    # create population parameters for data generation
    beta0 <- 0 # average y at t0 
    beta1 <- 0 # average increase for x=0
    beta2H0 <- 0 # average difference in slopes between conditions under H0
    beta2H1 <- eff.size * sqrt(sigmasq.u1) # average difference in slopes between conditions under H1
    
      for (i in 1:m) {
      #seeds[[i]] <- .Random.seed
      multinorm <- mvrnorm(n=N, mu=c(0,0), matrix(c(sigmasq.u0, sigma.u0.u1, sigma.u0.u1, sigmasq.u1), nrow=2, ncol=2)) # draw individual deviation from treatment intercept and slope from a multivariate normal distribution with mean 0.
      u0 <- rep(multinorm[,1], each=n)
      u1 <- rep(multinorm[,2], each=n)
      e <- rnorm(N*n, 0, sqrt(sigmasq.e))
      
      yH0 <- beta0 + u0 + beta1*t + beta2H0*treat*t + u1*t + e # data-generating mechanism under H0
      yH1 <- beta0 + u0 + beta1*t + beta2H1*treat*t + u1*t + e # data-generating mechanism under H1
      # datH0 <- data.frame(dat0, yH0)
      datH1 <- data.frame(dat0, yH1)

      # if (max(prop.bf01.H0)<eta) {
       modelsH0[[j]][[i]] <- lmer(yH0 ~ t + t:treat + (t | id), data = datH0, control = lmerControl(calc.derivs = F))
       estH0 <- modelsH0[[j]][[i]]@beta[3]
       names(estH0) <- c("t:treat")
       sigH0 <- list(as.matrix(vcov(modelsH0[[j]][[i]])[3,3]))
       resultH0 <- bain(estH0, hypotheses <- "t:treat>0;t:treat=0", n = N*n, Sigma = sigH0, group_parameters = 1, joint_parameters = 0)

      # bf10.H0[i] <- resultH0$BFmatrix[1,2]
       bf01.H0[i] <- resultH0$BFmatrix[2,1] #
       bf.1u.H0[i] <- resultH0$fit$BF.u[1] #
      #}

      modelsH1[[j]][[i]] <- lmer(yH1 ~ t + t:treat + (t | id), data = datH1, control = lmerControl(calc.derivs = F))

      #if (isSingular(models[[j]][[i]])) {next} # uncomment if singular models are not to be included

      estH1 <- modelsH1[[j]][[i]]@beta[3]
      names(estH1) <- c("t:treat")
      sigH1 <- list(as.matrix(vcov(modelsH1[[j]][[i]])[3,3]))
      resultH1 <- bain(estH1, hypotheses <- "t:treat>0;t:treat=0", n = N*n, Sigma = sigH1, group_parameters = 1, joint_parameters = 0) 
      
      bf10.H1[i] <- resultH1$BFmatrix[1,2]
      # bf01.H1[i] <- resultH1$BFmatrix[2,1]
      bf.1u.H1[i] <- resultH1$fit$BF.u[1]

    }
    
    # medbf10.H0[j] <- median(bf10.H0, na.rm=T)
    # prop.bf10.H0[j] <- length(bf10.H0[bf10.H0>BF.thresh])/m
    # prop.bf01.H0[j] <- length(bf01.H0[bf01.H0>BF.thresh])/m #
    # medbf01.H0[j] <- median(bf01.H0, na.rm=T) # 
    # medbf.1u.H0[j] <- median(bf.1u.H0, na.rm=T) #

    
    medbf10.H1[j] <- median(bf10.H1, na.rm=T)
    prop.bf10.H1[j] <- length(bf10.H1[bf10.H1>BF.thresh])/m
    # prop.bf01.H1[j] <- length(bf01.H1[bf01.H1>BF.thresh])/m
    # medbf01.H1[j] <- median(bf01.H1, na.rm=T)
    medbf.1u.H1[j] <- median(bf.1u.H1, na.rm=T)

    
    # if (max(prop.bf01.H0)>eta & max(prop.bf10.H1)>eta) {break}
    #if (max(prop.bf01.H0)>eta) {break}
    if (max(prop.bf10.H1)>eta) {break}

    print(prop.bf10.H1)
  }
  
  print(Sys.time() - start) 
  print(N)
  
  par(mfrow=c(2,3))
  plot(x=ss.seq, y=medbf10.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF10 under H0")
  plot(x=ss.seq, y=medbf.1u.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF unconstr. under H0")
  plot(x=ss.seq, y=prop.bf01.H0[1:length(ss.seq)], type="l", xlab="N", ylab="proportion of BFs larger 3 under H0")
  abline(h=eta, col="red")
  
  plot(x=ss.seq, y=medbf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF10 under H1")
  plot(x=ss.seq, y=medbf.1u.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF unconstr. under H1")
  plot(x=ss.seq, y=prop.bf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="proportion of BFs larger 3 under H1")
  abline(h=eta, col="red")
#}
  

  
  
# pdf(file = paste("c://", u, ".pdf", sep=""), width = 6, height = 6)
#   
#   par(mfrow=c(2,4))
#   plot(x=ss.seq, y=medbf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF10 under H1")
#   plot(x=ss.seq, y=medbf01.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF01 under H1")
#   plot(x=ss.seq, y=medbf.1u.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF unconstr. under H1")
#   plot(x=ss.seq, y=medbf.1c.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF complement under H1")
#   
#   plot(x=ss.seq, y=medpmp.a1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPa under H1")
#   plot(x=ss.seq, y=medpmp.b1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPb under H1")
#   plot(x=ss.seq, y=medpmp.c1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPc under H1")
#   plot(x=ss.seq, y=prop.bf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="proportion of BFs larger 3 under H1")
#   abline(h=eff.size, col="red")
# 
# dev.off()
  
  
#}

```


```{r}
df.prop.bf10.H1 <- as.data.frame(cbind(prop.bf10.H1[1:length(na.exclude(ss.seq))], na.exclude(ss.seq)))
colnames(df.prop.bf10.H1) <- c("prop.bf", "N")

# increase in proportion of BFs larger than BFthres
pdf(file = "C://Users/losen002/OneDrive - Universiteit Utrecht/Desktop/PhD/BayesianSSD/Prop.BF.medium.pdf", width = 6, height = 6)
ggplot(data=df.prop.bf10.H1, aes(x=N, y=prop.bf), linewidth=1.5) + 
  geom_line() +
  geom_hline(yintercept=eta, col="red", linetype="dashed") +
  labs(x="Proportion of BF01 larger than 3") 
dev.off()

# Sampling distribution of BFs for N
  
#bf10.H1.trim <- bf10.H1 #[bf10.H1<=quantile(bf10.H1, probs = .99)]
df.prop.bfs <- data.frame(c(bf10.H1, bf01.H0), rep(c("1","0"), each=m))
colnames(df.prop.bfs) <- c("BF", "hyp")
```

```{r}
# H0
p0 <- ggplot() + geom_density(aes(x=bf01.H0), linewidth=1) + 
  xlim(-3, 220) + 
  labs(title = "Sampling Distribution of BF01 under H0 with N = 20", y="density", x="BF01") 

dpb0 <- ggplot_build(p0)

p0 + geom_area(data=data.frame(x=dpb0$data[[1]]$x[dpb0$data[[1]]$x>3], y=dpb0$data[[1]]$y[dpb0$data[[1]]$x>3]), aes(x=x, y=y), fill="grey") +
    annotate("text", x=c(-3,25), y=c(.015,0.005), label=c(TeX("$BF_{thresh} = 3$"), TeX("$\\eta = .80$")), angle=c(90,0)) +
    geom_vline(xintercept=3, linetype=2, color="red", linewidth=1)

# H1
p1 <- ggplot() + geom_density(aes(x=bf10.H1), linewidth=1) + 
  xlim(-3, 220) + 
  labs(title = "Sampling Distribution of BF10 under H1 with N = 116", y="density", x="BF10") 

dpb1 <- ggplot_build(p1)

p1 + geom_area(data=data.frame(x=dpb1$data[[1]]$x[dpb1$data[[1]]$x>3], y=dpb1$data[[1]]$y[dpb1$data[[1]]$x>3]), aes(x=x, y=y), fill="grey") +
    annotate("text", x=c(-3,25), y=c(.01,0.0025), label=c(TeX("$BF_{thresh} = 3$"), TeX("$\\eta = .80$")), angle=c(90,0)) +
    geom_vline(xintercept=3, linetype=2, color="red", linewidth=1) 
```



# H0
bf01.H0.trim <- bf01.H0 #bf01.H0[bf01.H0<=quantile(bf01.H0, probs = .85)]
  
  plot(density(bf01.H0.trim), xlim=range(0:50))
  polygon(c(density(bf01.H0.trim)$x[density(bf01.H0.trim)$x >= 3 ], 3),
        c(density(bf01.H0.trim)$y[density(bf01.H0.trim)$x >= 3 ], 0),
        col = "slateblue1",
        border = 1)

p0 <- ggplot() + geom_density(aes(x=bf01.H0.trim), size=1) + 
  xlim(0, 50) + 
  geom_vline(xintercept=3, linetype=2, color="red") +
  annotate("text", x=2, y=.02, label="BFthresh=3", angle=90) +
  labs(title = TeX(r"(Sampling Distribution of $BF_{01}$ under $H_0$ for $N=20$)"), y="density", x=TeX("$BF_{01}$"))

dpb0 <- ggplot_build(p0)

p0 + geom_area(data=data.frame(x=dpb0$data[[1]]$x[dpb0$data[[1]]$x>3],
                       y=dpb0$data[[1]]$y[dpb0$data[[1]]$x>3]),
            aes(x=x, y=y), fill="grey")

```{r}

# both

pdf(file = "C://Users/losen002/OneDrive - Universiteit Utrecht/Desktop/PhD/BayesianSSD/SampDistH1.pdf", width = 6, height = 6)

p + geom_area(data=data.frame(x=dpb$data[[1]]$x[dpb$data[[1]]$x>3],
                       y=dpb$data[[1]]$y[dpb$data[[1]]$x>3]),
            aes(x=x, y=y), fill="grey")
dev.off()

pdf(file = "C://Users/losen002/OneDrive - Universiteit Utrecht/Desktop/PhD/BayesianSSD/SampDistH0.pdf", width = 6, height = 6)
p0 + geom_area(data=data.frame(x=dpb0$data[[1]]$x[dpb0$data[[1]]$x>3],
                       y=dpb0$data[[1]]$y[dpb0$data[[1]]$x>3]),
            aes(x=x, y=y), fill="grey")
dev.off()

# StatAreaUnderDensity <- ggproto(
#   "StatAreaUnderDensity", Stat,
#   required_aes = "x",
#   compute_group = function(data, scales, xlim = NULL, n = 50) {
#     fun <- approxfun(density(data$x))
#     StatFunction$compute_group(data, scales, fun = fun, xlim = xlim, n = n)
#   }
# )
# 
# stat_aud <- function(mapping = NULL, data = NULL, geom = "area",
#                     position = "identity", na.rm = FALSE, show.legend = NA, 
#                     inherit.aes = TRUE, n = 50, xlim=NULL,  
#                     ...) {
#   layer(
#     stat = StatAreaUnderDensity, data = data, mapping = mapping, geom = geom, 
#     position = position, show.legend = show.legend, inherit.aes = inherit.aes,
#     params = list(xlim = xlim, n = n, ...))
# }

```


```{r}
### SINGULARITY DIAGNOSTICS

marker <- rep(NA, m)
for(i in 1:m){
  marker[i] <- ifelse(isSingular(modelsH0[[1]][[i]]), 1, 0)
}

badmodels <- modelsH0[[1]][marker==1]
goodmodels <- modelsH0[[1]][marker==0]

badest <- rep(NA, length(badmodels))
badstderr <- rep(NA, length(badmodels))
for(i in 1:length(badmodels)){
badest[i] <- badmodels[[i]]@beta[3]
badstderr[i] <- diag(vcov(badmodels[[i]]))[3]
}

goodest <- rep(NA, length(goodmodels))
goodstderr <- rep(NA, length(goodmodels))
for(i in 1:length(goodmodels)){
goodest[i] <- goodmodels[[i]]@beta[3]
goodstderr[i] <- diag(vcov(goodmodels[[i]]))[3]
}

length(goodmodels)
length(badmodels)

summary(goodest)
summary(badest)
summary(goodstderr)
summary(badstderr)
```



```{r PDFs}

pdf(file = "C://Users/losen002/OneDrive - Universiteit Utrecht/Desktop/PhD/BayesianSSD/BF_PMP_H0.pdf",   
    width = 6, # The width of the plot in inches
    height = 6) # The height of the plot in inches

  par(mfrow=c(2,4))
  plot(x=ss.seq, y=medbf10.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF10 under H0")
  plot(x=ss.seq, y=medbf01.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF01 under H0")
  plot(x=ss.seq, y=medbf.1u.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF unconstr. under H0")
  plot(x=ss.seq, y=medbf.1c.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median BF complement under H0")
  
  plot(x=ss.seq, y=medpmp.a1.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPa under H0")
  plot(x=ss.seq, y=medpmp.b1.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPb under H0")
  plot(x=ss.seq, y=medpmp.c1.H0[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPc under H0")
  plot(x=ss.seq, y=prop.bf01.H0[1:length(ss.seq)], type="l", xlab="N", ylab="proportion of BFs larger 3 under H0")
  abline(h=.8, col="red")

dev.off()


pdf(file = "C://Users/losen002/OneDrive - Universiteit Utrecht/Desktop/PhD/BayesianSSD/BF_PMP_H1_medium.pdf",   
    width = 6, # The width of the plot in inches
    height = 6) # The height of the plot in inches

  par(mfrow=c(2,4))
  plot(x=ss.seq, y=medbf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF10 under H1")
  plot(x=ss.seq, y=medbf01.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF01 under H1")
  plot(x=ss.seq, y=medbf.1u.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF unconstr. under H1")
  plot(x=ss.seq, y=medbf.1c.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median BF complement under H1")
  
  plot(x=ss.seq, y=medpmp.a1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPa under H1")
  plot(x=ss.seq, y=medpmp.b1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPb under H1")
  plot(x=ss.seq, y=medpmp.c1.H1[1:length(ss.seq)], type="l", xlab="N", ylab="median PMPc under H1")
  plot(x=ss.seq, y=prop.bf10.H1[1:length(ss.seq)], type="l", xlab="N", ylab="proportion of BFs larger 3 under H1")
  abline(h=eff.size, col="red")

dev.off()

```


The resulting Bayes Factors tell us that $H_1$ is the hypothesis most likely to be true out of the set of considered hypotheses. We therefore conclude that in the treatment group ($X_1$), the symptom level increases more over time compared to the control group ($X_0$).

## Descriptives/plots

```{r Descriptives, echo=F}
# plot individual trajectories
ggplot(data = dat, aes(x = t, y = y, group = id, color = as.factor(treat))) + geom_line()

boxplot(formula = y ~ treat, data = dat) # in a boxplot
```

As can be seen from the figure, individuals differ in their intercept and slope. Subjects in the treatment group ($X_1$) seem to have systematically higher slopes as compared to those in the control group ($X_0$), suggesting an interaction effect between time and treatment. Also, the variability of y scores seems to increase over time, rendering the assumption of compound symmetry untenable. 


```{r Plots, echo=F}
# one line for everyone
ggplot(data = dat, aes(x = t, y = y), color = treat) + 
  geom_jitter(aes(color = treat), width = .1, height = 0) + 
  geom_smooth(method = "lm", formula = 'y ~ x')
# base
p <- ggplot(data = dat, aes(x = t, y = y, group = id, color = treat)) + geom_jitter(width = .1, height = 0)
# different lines for treat
p  + geom_smooth(group = 0, method = "lm", data = subset(dat, dat$treat == 0), formula = 'y ~ x') + geom_smooth(group = 0, method = "lm", data = subset(dat, dat$treat == 1), formula = 'y ~ x')

```

## Intercept only model

```{r Intercept only}
int.only <- lmer(y ~ 1 + (1 | id), data = dat)
summary(int.only) 
logLik(int.only)           
performance::icc(int.only) 
p + geom_smooth(method = "lm", formula = y ~ 1, se = F)
```

--------------------------------------------------------------------------------

## Appendix: In-between models

### Level 1 predictor: time

```{r Lvl 1}
lvl1 <- lmer(y ~ time + (1 | id), data = dat)
summary(lvl1)
logLik(lvl1)           
anova(int.only, lvl1)  
performance::icc(lvl1) 
```


### Level 2 predictor: treat

```{r Lvl 2}
lvl2 <- lmer(y ~ time + treat + (1 | id), data = dat)
summary(lvl2)
logLik(lvl2)           # -1063.76 (df=5) - better fit compared to lvl1 predictor only
anova(lvl1, lvl2)      # likelihood ratio test significant
performance::icc(lvl2) # 0.044
vcov.lmerMod(lvl2)
```

### Random slope for time

```{r Random slope}
rand.slop <- lmer(y ~ time + treat + (1 + time | id), data = dat)
summary(rand.slop)
logLik(rand.slop) 
anova(lvl2, rand.slop)
performance::icc(rand.slop)
```
